{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import qiime2 as q2\n",
    "import pandas as pd\n",
    "from biom import Table, load_table\n",
    "from biom.util import biom_open\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running songbird\n",
    "\n",
    "This is best done on a compute cluster but the scripts are provided in `diff-analysis-new/songbird-helper-new.py`. From these grid searches we determined the best perams. to run songbird with with a $Q^{2}$-value close to one. From these runs we can use the differentials to explore what microbes change between the birth-modes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Key accuracy/cv_error was not found in Reservoir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c90fd790b3ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevent_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy/cv_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2019.7/lib/python3.6/site-packages/tensorboard/backend/event_processing/event_accumulator.py\u001b[0m in \u001b[0;36mScalars\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    480\u001b[0m           \u001b[0mAn\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScalarEvent\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2019.7/lib/python3.6/site-packages/tensorboard/backend/event_processing/reservoir.py\u001b[0m in \u001b[0;36mItems\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key %s was not found in Reservoir\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Key accuracy/cv_error was not found in Reservoir'"
     ]
    }
   ],
   "source": [
    "event_acc.Scalars('accuracy/cv_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Key accuracy/cv_error was not found in Reservoir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8a8672a6b8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mevent_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# get scalar perams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mw_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevent_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy/cv_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mbaseline_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# get all fomrula based models CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2019.7/lib/python3.6/site-packages/tensorboard/backend/event_processing/event_accumulator.py\u001b[0m in \u001b[0;36mScalars\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    480\u001b[0m           \u001b[0mAn\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScalarEvent\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2019.7/lib/python3.6/site-packages/tensorboard/backend/event_processing/reservoir.py\u001b[0m in \u001b[0;36mItems\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key %s was not found in Reservoir\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Key accuracy/cv_error was not found in Reservoir'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# for each body site repeat \n",
    "all_grid_results = {}\n",
    "input_path = '../data/diff-analysis-new'\n",
    "days = [\"2.0\", \"30.0\", \"120.0\", \"180.0\"]\n",
    "# get all body site path(s)\n",
    "partition_path = os.path.join(input_path, 'songbird-grid-search/*')\n",
    "body_sites = [bs_.split('/')[-1]\n",
    "                  for bs_ in glob.glob(partition_path)]\n",
    "# run for each body site\n",
    "for body_site_ in body_sites:\n",
    "    # for body site subsets\n",
    "    sub_path = os.path.join(input_path, 'songbird-grid-search', body_site_, '*')\n",
    "\n",
    "    # get all baseline models CV\n",
    "    baseline_ls_path =  os.path.join(input_path, 'songbird-grid-search',\n",
    "                                     body_site_, '1*')\n",
    "    baseline_models = glob.glob(baseline_ls_path)\n",
    "    baseline_models = {tuple(id_.split('/')[-1].split('-')[1:]):id_\n",
    "                       for id_ in baseline_models}\n",
    "    # retrieve all baseline models\n",
    "    for id_, path_ in baseline_models.items():\n",
    "        # get path to data\n",
    "        event_acc = EventAccumulator(path_)\n",
    "        event_acc.Reload()\n",
    "        # get scalar perams\n",
    "        w_times, step_nums, vals = zip(*event_acc.Scalars('accuracy/cv_error'))\n",
    "        baseline_models[id_] = [w_times, step_nums, vals]\n",
    "    # get all fomrula based models CV\n",
    "    all_ls_path =  os.path.join(input_path, 'songbird-grid-search',\n",
    "                                body_site_, '*')\n",
    "    formula_models = glob.glob(all_ls_path)\n",
    "    exclude_ = glob.glob(baseline_ls_path)\n",
    "    formula_models = sorted(set(formula_models) - set(exclude_))\n",
    "    formula_models = {tuple(id_.split('/')[-1].split('-')[:]):id_\n",
    "                      for id_ in formula_models}\n",
    "    formula_models = {('-'.join(k[:-3]),k[-3],k[-2],k[-1]):v\n",
    "                      for k,v in formula_models.items()}\n",
    "    for id_, path_ in formula_models.items():\n",
    "        # get path to data\n",
    "        event_acc = EventAccumulator(path_)\n",
    "        event_acc.Reload()\n",
    "        # get scalar perams\n",
    "        w_times, step_nums, vals = zip(*event_acc.Scalars('accuracy/cv_error'))\n",
    "        # calc q^2-value\n",
    "        base_cv = np.mean(baseline_models[id_[1:]][-1][-10:])\n",
    "        form_cv = np.mean(vals[-10:])\n",
    "        q_squared = 1 - form_cv/base_cv\n",
    "        formula_models[id_] = [form_cv, base_cv, q_squared]\n",
    "    # make dataframe to save\n",
    "    gird_results = pd.DataFrame(formula_models).T.reset_index()\n",
    "    gird_results.columns = ['formula', 'min_features', 'batch_size',\n",
    "                            'differential_prior', 'CV', 'baseline_CV',\n",
    "                            'q_squared']\n",
    "    all_grid_results[body_site_] = gird_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4', '4', '0.5')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get best\n",
    "all_grid_df = pd.concat(all_grid_results).reset_index().drop('level_1', axis=1)\n",
    "all_grid_df[['body_site', 'days']] = all_grid_df.level_0.str.rsplit(pat = \"-\", n = 1, expand = True)\n",
    "all_grid_df_allowed = all_grid_df.drop('level_0', axis = 1)[all_grid_df.q_squared > 0].copy()\n",
    "ind_ = all_grid_df_allowed.groupby(['body_site','days','formula'])[['CV']].idxmin().values\n",
    "all_grid_df_best = all_grid_df_allowed.loc[ind_.flatten(), :]\n",
    "all_grid_df_best.to_csv('../data/diff-analysis-new/Songbird-modeling.tsv', sep='\\t')\n",
    "all_grid_df_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_ = '../data/diff-analysis-new/songbird-grid-search/'\n",
    "file_path_copy_ = '../data/diff-analysis-new/songbird-optimized-models/'\n",
    "\n",
    "paths_copy_ = [(os.path.join(file_path_, '-'.join([bs_, k_]),\n",
    "                             '-'.join(df2_.values[0][0:4])),\n",
    "                os.path.join(file_path_copy_, '-'.join([bs_, k_]),\n",
    "                             '-'.join(df2_.values[0][0:4])),\n",
    "                os.path.join(file_path_, '-'.join([bs_, k_]),\n",
    "                             '1-' + '-'.join(df2_.values[0][1:4])),\n",
    "                os.path.join(file_path_copy_, '-'.join([bs_, k_]),\n",
    "                             '1-' + '-'.join(df2_.values[0][1:4])))\n",
    "               for bs_, bsdf_ in all_grid_df_best.groupby('body_site')\n",
    "               for k_, df_ in bsdf_.groupby('days')\n",
    "               for k2_, df2_ in df_.groupby('formula')]\n",
    "# copy the paths\n",
    "for copy_ in paths_copy_:\n",
    "    if not os.path.exists(copy_[1]):\n",
    "        shutil.copytree(copy_[0], copy_[1])\n",
    "        print(copy_[1])\n",
    "    if not os.path.exists(copy_[3]):\n",
    "        shutil.copytree(copy_[2], copy_[3])\n",
    "        print(copy_[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from skbio.stats.composition import (alr_inv, alr,\n",
    "                                     closure, clr)\n",
    "# warnings filter \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import taxonomy\n",
    "taxdf = q2.Artifact.load('../data/processed-data/taxonomy.qza').view(q2.Metadata).to_dataframe()\n",
    "\n",
    "def split_taxonomy(taxonomy):\n",
    "    feat_map = dict(taxonomy.Taxon)\n",
    "    taxonomy['Taxon'] = [feat_map[feat]\n",
    "                         if feat in feat_map.keys()\n",
    "                         else np.nan\n",
    "                         for feat in taxonomy.index]\n",
    "    # add taxonomic levels for grouping later (if available)\n",
    "\n",
    "    def tax_split(tax_id, tax_level): return tax_id.split(\n",
    "        tax_level)[1].split(';')[0]\n",
    "\n",
    "    for level, lname in zip(['k__', 'p__', 'c__', 'o__',\n",
    "                             'f__', 'g__', 's__'],\n",
    "                            ['kingdom', 'phylum', 'class',\n",
    "                             'order', 'family', 'genus',\n",
    "                             'species']):\n",
    "        if lname not in taxonomy.columns:\n",
    "            taxonomy_tmp = []\n",
    "            for tax in taxonomy.Taxon:\n",
    "                if tax is not np.nan and\\\n",
    "                   level in tax and\\\n",
    "                   len(tax_split(tax, level)) > 0:\n",
    "                    taxonomy_tmp.append(tax_split(tax,\n",
    "                                                  level))\n",
    "                else:\n",
    "                    taxonomy_tmp.append(np.nan)\n",
    "            taxonomy[lname] = taxonomy_tmp\n",
    "    return taxonomy\n",
    "\n",
    "# split the levels into columns\n",
    "taxdf = split_taxonomy(taxdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_differentials(clr_diff):\n",
    "    \"\"\" re-centers data around zero \"\"\"\n",
    "    \n",
    "    # center again around zero after completion\n",
    "    clr_diff = clr_diff \\\n",
    "                - clr_diff.mean(axis=0).values\n",
    "    clr_diff = clr_diff \\\n",
    "                - clr_diff.mean(axis=1).values.reshape(-1, 1)\n",
    "    # return the re-centered data\n",
    "    return clr_diff\n",
    "\n",
    "def differentials_to_probability(differentials,\n",
    "                                 numerators,\n",
    "                                 prefix='C(birth_mode, Treatment(\"CS\"))',\n",
    "                                 basis_name=\"P(CS)\"):\n",
    "    \"\"\" converts differentials to something like\n",
    "        a probability using the inverse-alr transform.\n",
    "    \"\"\"\n",
    "    # first recenter the differentials\n",
    "    differentials = center_differentials(differentials)\n",
    "    # then take the inverse alr\n",
    "    prob_differentials = alr_inv(differentials[numerators])\n",
    "    # make a dataframe to return \n",
    "    columns = [col.replace(\"[T.\",\"P(\").replace(\"]\",\")\").replace(prefix, \"\")\n",
    "                for col in numerators] # rename cols\n",
    "    columns = [basis_name] + columns\n",
    "    prob_differentials = pd.DataFrame(prob_differentials,\n",
    "                                      differentials.index,\n",
    "                                      columns)\n",
    "    return prob_differentials\n",
    "\n",
    "# container for differentials\n",
    "all_differentials = {}\n",
    "all_metadata = {}\n",
    "all_tables = {}\n",
    "# get path info\n",
    "data_path = '../data/diff-analysis-new'\n",
    "input_path = '../data/diff-analysis-new/songbird-optimized-models/'\n",
    "diffs_use = ['[T.CS]', '[T.CSseed]']\n",
    "# add a frequency filter\n",
    "min_freq = 0.0\n",
    "# get all body site path(s)\n",
    "partition_path = os.path.join(input_path, '*')\n",
    "body_sites = [bs_.split('/')[-1]\n",
    "                  for bs_ in glob.glob(partition_path)]\n",
    "# run for each body site\n",
    "for body_site_ in body_sites:\n",
    "    # for body site subsets\n",
    "    sub_path = os.path.join(input_path, body_site_, '*')\n",
    "\n",
    "    # get differentials\n",
    "    baseline_ls_path = os.path.join(input_path, body_site_, '1*')\n",
    "    formula_models = glob.glob(sub_path)\n",
    "    exclude_ = glob.glob(baseline_ls_path)\n",
    "    formula_models = sorted(set(formula_models) - set(exclude_))[0]\n",
    "    # get diffs. and add taxonomy labels\n",
    "    diff = pd.read_csv(os.path.join(formula_models, 'differentials.tsv'),\n",
    "                       sep='\\t', index_col=0)\n",
    "    # get table and metadata for each subset\n",
    "    data_split_path = os.path.join(data_path, body_site_)\n",
    "    mf = pd.read_csv(os.path.join(data_split_path,\n",
    "                                  'metadata.tsv'),\n",
    "                       sep='\\t', index_col=0)\n",
    "    bt = load_table(os.path.join(data_split_path,\n",
    "                                 'table.biom'))\n",
    "    # caclulate the freq. of that feature in the data, the number of times a feature appeared\n",
    "    # in the sampels\n",
    "    frequncy = pd.DataFrame(bt.matrix_data.toarray().astype(bool).sum(1) / bt.shape[1],\n",
    "                    bt.ids('observation'), ['feature-frequency']) \n",
    "    frequncy = frequncy.reindex(diff.index)\n",
    "    # add to diff to reduce files saved\n",
    "    diff['feature-frequency'] = frequncy['feature-frequency']\n",
    "    # apply the filter\n",
    "    diff = diff[diff['feature-frequency'] >= min_freq]\n",
    "    # reindex taxonomy\n",
    "    taxdf_ = taxdf.reindex(diff.index)\n",
    "    # get the prob. of each microbe in each state\n",
    "    differential_cols = [col_ for col_ in diff.columns\n",
    "                         if any(dc in col_ for dc in diffs_use)]\n",
    "    pdiff = differentials_to_probability(diff[['Intercept'] + differential_cols],\n",
    "                                         differential_cols,\n",
    "                                        prefix='C(birth_mode_ms, Treatment(\"Vag\"))',\n",
    "                                        basis_name=\"P(Vag)\")\n",
    "    pdiff = pdiff.reindex(diff.index)\n",
    "    # add all together\n",
    "    diff = pd.concat([pdiff, diff, taxdf_], axis=1)\n",
    "    # calculate seeding effectiveness metric TBA\n",
    "    diff['seeding-effectiveness'] = (diff['P(Vag)'] * diff['P(CSseed)']) + (diff['P(CS)']/4)\n",
    "    # save the files\n",
    "    all_differentials[body_site_] = diff.rename({'P(CSseed)':'P(CS-seeded)',\n",
    "                                                                'P(Vag)':'P(Vaginal)'}, axis=1)\n",
    "    all_metadata[body_site_] = mf\n",
    "    all_tables[body_site_] = bt\n",
    "    if body_site_=='Baby-Feces-2.0':\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(all_differentials).reset_index().rename(columns = {'level_0':'body_site_days'}).to_csv(\n",
    "    os.path.join(data_path, 'Songbird-final.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
